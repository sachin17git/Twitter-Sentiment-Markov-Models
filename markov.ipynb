{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment detection using Markov Models.\n",
    "- First-order markov\n",
    "- Second-order markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "stopwords = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>and and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice talk to your neighbour family to exchan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>coronavirus australia woolworth to give elderl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>my food stock be not the only one which be emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>me ready to go at supermarket during the outbr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "                                              tweets  \n",
       "0                                            and and  \n",
       "1  advice talk to your neighbour family to exchan...  \n",
       "2  coronavirus australia woolworth to give elderl...  \n",
       "3  my food stock be not the only one which be emp...  \n",
       "4  me ready to go at supermarket during the outbr...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_unwanted(x):\n",
    "    new_x = re.sub(r'<[^>]*>', '', x) #remove html tags\n",
    "    new_x = re.sub(r\"http\\S+\", \"\", new_x) #remove http links\n",
    "    new_x = re.sub(r\"@\\S+\", \"\", new_x) #remove twitter user tags\n",
    "    new_x = re.sub(r\"#\\S+\", \"\", new_x) #remove #tags\n",
    "    new_x = new_x.translate(str.maketrans('', '', string.punctuation))\n",
    "    new_x = \" \".join([txt for txt in new_x.lower().split() if len(txt.strip()) > 0])\n",
    "    x_tags = pos_tag(new_x.split()) # get part-of-speech tags for each token in a text.\n",
    "    new_x = \" \".join([lemmatizer.lemmatize(m, pos = detect_pos_tag(tag)) for m, tag in x_tags]) # lemmatize based on the POS tags.\n",
    "    \n",
    "    return new_x\n",
    "\n",
    "def detect_pos_tag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "data = pd.read_csv(\"../corona tweets/Corona_NLP_train.csv\", encoding='ISO-8859-1')\n",
    "data.drop([\"UserName\",\"ScreenName\", \"Location\", \"TweetAt\"], axis=1, inplace=True)\n",
    "data[\"tweets\"] = data[\"OriginalTweet\"].apply(remove_unwanted)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove tweets of length less than 1. Let's consider **extremely positive** as positive and **extremely negative** as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice talk to your neighbour family to exchan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>coronavirus australia woolworth to give elderl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>my food stock be not the only one which be emp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>me ready to go at supermarket during the outbr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>a news of the regionâs first confirm covid19 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "5  As news of the regionÂs first confirmed COVID...            Positive   \n",
       "\n",
       "                                              tweets  labels  \n",
       "1  advice talk to your neighbour family to exchan...       0  \n",
       "2  coronavirus australia woolworth to give elderl...       0  \n",
       "3  my food stock be not the only one which be emp...       0  \n",
       "4  me ready to go at supermarket during the outbr...       1  \n",
       "5  a news of the regionâs first confirm covid19 ...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data[\"tweets\"].str.len() > 1]\n",
    "\n",
    "codes = {\n",
    "    \"Positive\": 0,\n",
    "    \"Neutral\": 2,\n",
    "    \"Negative\": 1\n",
    "}\n",
    "\n",
    "def label_changer(y):\n",
    "    if y == \"Extremely Positive\":\n",
    "        return codes[\"Positive\"]\n",
    "    elif y == \"Extremely Negative\":\n",
    "        return codes[\"Negative\"]\n",
    "    else:\n",
    "        return codes[y]\n",
    "\n",
    "data[\"labels\"] = data[\"Sentiment\"].apply(label_changer)\n",
    "\n",
    "data = data[data[\"labels\"] < 2]\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this model, consider only **positive** and **negative** reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18044\n",
      "1    15397\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data[\"labels\"].value_counts())\n",
    "plt.figure(figsize=(7, 5), dpi = 100)\n",
    "sns.countplot(data['labels']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data is not perfectly balanced.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"tweets\"].tolist()\n",
    "y = data[\"labels\"].tolist()\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y,\n",
    "                                                stratify = y,\n",
    "                                                test_size = 0.05, \n",
    "                                                random_state = 17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a mapping between word and id, ***word --> id***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32173"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1\n",
    "word2idx = {\"<unk>\": 0}\n",
    "\n",
    "for text in xtrain:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        if token not in word2idx:\n",
    "            word2idx[token] = idx\n",
    "            idx += 1\n",
    "\n",
    "len(word2idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = len(word2idx)\n",
    "V"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a reverse mapping ***id --> word***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {j:i for i, j in word2idx.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all the text tokens into ids using word2idx created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_int = []\n",
    "xtest_int = []\n",
    "\n",
    "for text in xtrain:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    tokens = text.split()\n",
    "    line_int = [word2idx[token] for token in tokens]\n",
    "    xtrain_int.append(line_int)\n",
    "\n",
    "for text in xtest:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    tokens = text.split()\n",
    "    line_int = [word2idx.get(token, 0) for token in tokens]\n",
    "    xtest_int.append(line_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text converted to id --->\n",
      "\n",
      " [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 22, 25, 33, 34, 35, 36, 37, 38, 39, 32, 40, 22, 41, 6, 42, 43, 5, 35, 44, 45, 5, 46, 24, 23, 47, 22, 48, 49] \n",
      "\n",
      "Text --->\n",
      "\n",
      " we know you re worried about bill during covid 19 and we re work with our insurer on relief option and discount we ll be reach out to our client soon to let you know how we can help\n"
     ]
    }
   ],
   "source": [
    "print(\"Text converted to id --->\\n\\n\", xtrain_int[1], \"\\n\\nText --->\\n\\n\", xtrain[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-order Markov model\n",
    "#### We are creating seperate probabilistic model for each class.\n",
    "- initial_pos and initial_neg are the initial state distributions for positive and negative, which relates to the first word in a sentence.\n",
    "- first_order_pos (positive) and first_order_neg (negative) are the transition state matrix for each class, which contains log probabilities of transition between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_pos = {}\n",
    "first_order_pos = {}\n",
    "\n",
    "initial_neg = {}\n",
    "first_order_neg = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute counts/frequency for initial state and transition state matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(txt_to_int, initials, transitions):\n",
    "    for tokens in txt_to_int:\n",
    "        last_idx = None\n",
    "        for idx in tokens:\n",
    "            if last_idx is None:\n",
    "                initials[idx] = initials.get(idx, 1) + 1\n",
    "            else:\n",
    "                if last_idx not in transitions:\n",
    "                    transitions[last_idx] = {idx : 2}\n",
    "                else:\n",
    "                    transitions[last_idx][idx] = transitions[last_idx].get(idx, 1) + 1\n",
    "            \n",
    "            last_idx = idx\n",
    "    \n",
    "    return initials, transitions\n",
    "\n",
    "initial_pos, first_order_pos = compute_counts([x for x, y in zip(xtrain_int, ytrain) if y == 0], initial_pos, first_order_pos)\n",
    "initial_neg, first_order_neg = compute_counts([x for x, y in zip(xtrain_int, ytrain) if y == 1], initial_neg, first_order_neg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute word frequency for the entire vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(xx):\n",
    "    word_freq = {}\n",
    "    for text in xx:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        tokens = text.split()\n",
    "        for token in tokens:\n",
    "            if token not in word_freq:\n",
    "                word_freq[token] = 1\n",
    "            else:\n",
    "                word_freq[token] += 1\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "word_freq_pos = frequency([x for x, y in zip(xtrain, ytrain) if y == 0])\n",
    "word_freq_neg = frequency([x for x, y in zip(xtrain, ytrain) if y == 1])\n",
    "\n",
    "word_frequency = dict(Counter(word_freq_pos) + Counter(word_freq_neg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that calculates log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_logs(d, k):\n",
    "    new_d = {}\n",
    "    word = idx2word[k]\n",
    "    for nk, v in d.items():\n",
    "        new_d[nk] = np.log((v + 1) / word_frequency[word])\n",
    "    \n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_sum = sum(initial_pos.values())\n",
    "ineg_sum = sum(initial_neg.values())\n",
    "\n",
    "for key, val in initial_pos.items():\n",
    "    initial_pos[key] = np.log(val / ipos_sum)\n",
    "\n",
    "for key, val in initial_neg.items():\n",
    "    initial_neg[key] = np.log(val / ineg_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in first_order_pos.items():\n",
    "    first_order_pos[key] = transition_logs(val, key)\n",
    "\n",
    "for key, val in first_order_neg.items():\n",
    "    first_order_neg[key] = transition_logs(val, key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate prior log probabilities for each class (positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count0 = sum(y==0 for y in ytrain)\n",
    "count1 = sum(y==1 for y in ytrain)\n",
    "\n",
    "total = len(ytrain)\n",
    "\n",
    "p0 = count0 / total\n",
    "p1 = count1 / total\n",
    "\n",
    "logp0 = np.log(p0)\n",
    "logp1 = np.log(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17141, 14627)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count0, count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6169862393040303, -0.7755903579239991)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp0, logp1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a class which computes log likelihood & also design predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, logAs, logpis, logpriors, word_freq):\n",
    "        self.logAs = logAs\n",
    "        self.logpis = logpis\n",
    "        self.logpriors = logpriors\n",
    "        self.word_freq = word_freq\n",
    "        self.K = len(logpriors)\n",
    "    \n",
    "    def compute_log_likelihood(self, input_, class_):\n",
    "        logA = self.logAs[class_]\n",
    "        logpi = self.logpis[class_]\n",
    "        \n",
    "        last_idx = None\n",
    "        logprob = 0\n",
    "\n",
    "        for idx in input_:\n",
    "            if last_idx is None:\n",
    "                if idx in logpi:\n",
    "                    logprob += logpi[idx]\n",
    "            else:\n",
    "                if last_idx in logA:\n",
    "                    if idx in logA[last_idx]:\n",
    "                        logprob += logA[last_idx][idx]\n",
    "                    else:\n",
    "                        word = idx2word[last_idx]\n",
    "                        freq = self.word_freq[word]\n",
    "                        logprob += np.log(1 / freq)\n",
    "        \n",
    "            last_idx = idx\n",
    "        \n",
    "        return logprob\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        predictions = np.zeros(len(inputs))\n",
    "        for e, input_ in enumerate(inputs):\n",
    "            posteriors = [self.compute_log_likelihood(input_, c) + self.logpriors[c] for c in range(self.K)]\n",
    "            prediction = np.argmax(posteriors)\n",
    "            predictions[e] = prediction\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier([first_order_pos, first_order_neg],\n",
    "                 [initial_pos, initial_neg],\n",
    "                 [logp0, logp1],\n",
    "                  word_frequency)\n",
    "ptrain = clf.predict(xtrain_int)\n",
    "ptest = clf.predict(xtest_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96     17141\n",
      "           1       0.97      0.93      0.95     14627\n",
      "\n",
      "    accuracy                           0.95     31768\n",
      "   macro avg       0.95      0.95      0.95     31768\n",
      "weighted avg       0.95      0.95      0.95     31768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytrain, ptrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16659,   482],\n",
       "       [ 1065, 13562]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytrain, ptrain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.89      0.78       903\n",
      "           1       0.81      0.52      0.63       770\n",
      "\n",
      "    accuracy                           0.72      1673\n",
      "   macro avg       0.75      0.71      0.71      1673\n",
      "weighted avg       0.74      0.72      0.71      1673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, ptest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[808,  95],\n",
       "       [368, 402]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, ptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics --->\n",
      "\n",
      "F1-score - 0.946\n",
      "Precision - 0.966\n",
      "Recall - 0.927\n",
      "Auc - 0.950\n"
     ]
    }
   ],
   "source": [
    "print(\"Training metrics --->\\n\")\n",
    "\n",
    "print(f\"F1-score - {f1_score(ytrain, ptrain):.3f}\")\n",
    "print(f\"Precision - {precision_score(ytrain, ptrain):.3f}\")\n",
    "print(f\"Recall - {recall_score(ytrain, ptrain):.3f}\")\n",
    "print(f\"Auc - {roc_auc_score(ytrain, ptrain):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics --->\n",
      "\n",
      "F1-score - 0.635\n",
      "Precision - 0.809\n",
      "Recall - 0.522\n",
      "Auc - 0.708\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation metrics --->\\n\")\n",
    "print(f\"F1-score - {f1_score(ytest, ptest):.3f}\")\n",
    "print(f\"Precision - {precision_score(ytest, ptest):.3f}\")\n",
    "print(f\"Recall - {recall_score(ytest, ptest):.3f}\")\n",
    "print(f\"Auc - {roc_auc_score(ytest, ptest):.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-order Markov model\n",
    "- Here we have extra components second_order_pos and second_order_neg, they keep track upto 2 previous words for a current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_pos = {}\n",
    "first_order_pos = {}\n",
    "second_order_pos = {}\n",
    "\n",
    "initial_neg = {}\n",
    "first_order_neg = {}\n",
    "second_order_neg = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(txt_to_int, initials, transition_f, transition_s):\n",
    "    for tokens in txt_to_int:\n",
    "        for e, idx in enumerate(tokens):\n",
    "            if e == 0:\n",
    "                initials[idx] = initials.get(idx, 1) + 1\n",
    "            elif e == 1:\n",
    "                last_idx = tokens[e - 1]\n",
    "                if last_idx not in transition_f:\n",
    "                    transition_f[last_idx] = {idx : 2}\n",
    "                else:\n",
    "                    transition_f[last_idx][idx] = transition_f[last_idx].get(idx, 1) + 1\n",
    "            \n",
    "            elif e > 1:\n",
    "                prev_token2 = tokens[e - 2]\n",
    "                prev_token1 = tokens[e - 1]\n",
    "                last_idx = (prev_token2, prev_token1)\n",
    "                if last_idx not in transition_s:\n",
    "                    transition_s[last_idx] = {idx: 2}\n",
    "                else:\n",
    "                    transition_s[last_idx][idx] = transition_s[last_idx].get(idx, 1) + 1\n",
    "    \n",
    "    return initials, transition_f, transition_s\n",
    "\n",
    "initial_pos, first_order_pos, second_order_pos = compute_counts([x for x, y in zip(xtrain_int, ytrain) if y == 0],\n",
    "                                              initial_pos,\n",
    "                                              first_order_pos,\n",
    "                                              second_order_pos)\n",
    "\n",
    "initial_neg, first_order_neg, second_order_neg = compute_counts([x for x, y in zip(xtrain_int, ytrain) if y == 1],\n",
    "                                               initial_neg, \n",
    "                                               first_order_neg,\n",
    "                                               second_order_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo_frequency(d):\n",
    "    freq = {}\n",
    "    for key, val in d.items():\n",
    "        freq[key] = sum(val.values())\n",
    "    \n",
    "    return freq\n",
    "\n",
    "combo_freq_pos = combo_frequency(second_order_pos)\n",
    "combo_freq_neg = combo_frequency(second_order_neg)\n",
    "\n",
    "combo_freq = dict(Counter(combo_freq_pos) + Counter(combo_freq_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_transition_logs(d, k):\n",
    "    new_d = {}\n",
    "    for nk, v in d.items():\n",
    "        new_d[nk] = np.log((v + 1) / combo_freq[k])\n",
    "    \n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipos_sum = sum(initial_pos.values())\n",
    "ineg_sum = sum(initial_neg.values())\n",
    "\n",
    "for key, val in initial_pos.items():\n",
    "    initial_pos[key] = np.log(val / ipos_sum)\n",
    "\n",
    "for key, val in initial_neg.items():\n",
    "    initial_neg[key] = np.log(val / ineg_sum)\n",
    "\n",
    "for key, val in first_order_pos.items():\n",
    "    first_order_pos[key] = transition_logs(val, key)\n",
    "\n",
    "for key, val in first_order_neg.items():\n",
    "    first_order_neg[key] = transition_logs(val, key)\n",
    "\n",
    "for key, val in second_order_pos.items():\n",
    "    second_order_pos[key] = second_transition_logs(val, key)\n",
    "\n",
    "for key, val in second_order_neg.items():\n",
    "    second_order_neg[key] = second_transition_logs(val, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count0 = sum(y==0 for y in ytrain)\n",
    "count1 = sum(y==1 for y in ytrain)\n",
    "\n",
    "total = len(ytrain)\n",
    "\n",
    "p0 = count0 / total\n",
    "p1 = count1 / total\n",
    "\n",
    "logp0 = np.log(p0)\n",
    "logp1 = np.log(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6169862393040303, -0.7755903579239991)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp0, logp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, logSs, logAs, logpis, logpriors, word_f, combo_f):\n",
    "        self.logSs = logSs\n",
    "        self.logAs = logAs\n",
    "        self.logpis = logpis\n",
    "        self.logpriors = logpriors\n",
    "        self.word_freq = word_f\n",
    "        self.combo_freq = combo_f\n",
    "        self.K = len(logpriors)\n",
    "    \n",
    "    def compute_log_likelihood(self, input_, class_):\n",
    "        logS = self.logSs[class_]\n",
    "        logA = self.logAs[class_]\n",
    "        logpi = self.logpis[class_]\n",
    "        \n",
    "        logprob = 0\n",
    "\n",
    "        for e, idx in enumerate(input_):\n",
    "            if e == 0:\n",
    "                if idx in logpi:\n",
    "                    logprob += logpi[idx]\n",
    "            elif e == 1:\n",
    "                last_idx = input_[e - 1]\n",
    "                if last_idx in logA:\n",
    "                    if idx in logA[last_idx]:\n",
    "                        logprob += logA[last_idx][idx]\n",
    "                    else:\n",
    "                        word = idx2word[last_idx]\n",
    "                        freq = self.word_freq[word]\n",
    "                        logprob += np.log(1/freq)\n",
    "        \n",
    "            elif e > 1:\n",
    "                prev_token2 = input_[e - 2]\n",
    "                prev_token1 = input_[e - 1]\n",
    "                last_idx = (prev_token2, prev_token1)\n",
    "                if last_idx in logS:\n",
    "                    if idx in logS[last_idx]:\n",
    "                        logprob += logS[last_idx][idx]\n",
    "                    else:\n",
    "                        freq = self.combo_freq[last_idx]\n",
    "                        logprob += np.log(1 / freq)\n",
    "        \n",
    "        return logprob\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        predictions = np.zeros(len(inputs))\n",
    "        for e, input_ in enumerate(inputs):\n",
    "            posteriors = [self.compute_log_likelihood(input_, c) + self.logpriors[c] for c in range(self.K)]\n",
    "            prediction = np.argmax(posteriors)\n",
    "            predictions[e] = prediction\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier([second_order_pos, second_order_neg],\n",
    "                 [first_order_pos, first_order_neg],\n",
    "                 [initial_pos, initial_neg],\n",
    "                 [logp0, logp1],\n",
    "                 word_frequency,\n",
    "                 combo_freq)\n",
    "                 \n",
    "ptrain = clf.predict(xtrain_int)\n",
    "ptest = clf.predict(xtest_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     17141\n",
      "           1       0.95      0.95      0.95     14627\n",
      "\n",
      "    accuracy                           0.96     31768\n",
      "   macro avg       0.96      0.96      0.96     31768\n",
      "weighted avg       0.96      0.96      0.96     31768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytrain, ptrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16431,   710],\n",
       "       [  704, 13923]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytrain, ptrain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67       903\n",
      "           1       0.60      0.53      0.56       770\n",
      "\n",
      "    accuracy                           0.62      1673\n",
      "   macro avg       0.62      0.61      0.61      1673\n",
      "weighted avg       0.62      0.62      0.62      1673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, ptest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[632, 271],\n",
       "       [362, 408]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, ptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics --->\n",
      "\n",
      "F1-score - 0.952\n",
      "Precision - 0.951\n",
      "Recall - 0.952\n",
      "Auc - 0.955\n"
     ]
    }
   ],
   "source": [
    "print(\"Training metrics --->\\n\")\n",
    "\n",
    "print(f\"F1-score - {f1_score(ytrain, ptrain):.3f}\")\n",
    "print(f\"Precision - {precision_score(ytrain, ptrain):.3f}\")\n",
    "print(f\"Recall - {recall_score(ytrain, ptrain):.3f}\")\n",
    "print(f\"Auc - {roc_auc_score(ytrain, ptrain):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics --->\n",
      "\n",
      "F1-score - 0.563\n",
      "Precision - 0.601\n",
      "Recall - 0.530\n",
      "Auc - 0.615\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation metrics --->\\n\")\n",
    "print(f\"F1-score - {f1_score(ytest, ptest):.3f}\")\n",
    "print(f\"Precision - {precision_score(ytest, ptest):.3f}\")\n",
    "print(f\"Recall - {recall_score(ytest, ptest):.3f}\")\n",
    "print(f\"Auc - {roc_auc_score(ytest, ptest):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UB-DL",
   "language": "python",
   "name": "ub_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b09df8cc58716224621aaa1ca07c20ebfa5557089cdfdb3e9ac04078a58d2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
